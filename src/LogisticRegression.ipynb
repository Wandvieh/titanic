{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic dataset: Logistic Regression\n",
    "\n",
    "First step: Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold)\n",
    "from ISLP.models import sklearn_sm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../train.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "for col in df.columns:\n",
    "    print(\"Missing rows in {0}:\".format(col), df[col].shape[0] - df[col].count())\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting Sex and Embarked to a categorical value\n",
    "- Converting Sex to a new variable \"SexNr\", where 0 is male and 1 is female\n",
    "- Converting Cabin to a new variable \"CabinLetter\" with only the first letter used, then converting that letter into their corresponding number (e.g. A=1, B=2, etc.) until G=6, and then T=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = df['Sex'].astype('category')\n",
    "df['Embarked'] = df['Embarked'].astype('category')\n",
    "df['SexNr'] = df['Sex'].cat.codes\n",
    "df['CabinLetter'] = df.Cabin.str[:1]\n",
    "print(df.CabinLetter.unique())\n",
    "\n",
    "df['CabinLetter'] = df['CabinLetter'].astype('category')\n",
    "df['CabinLetter'] = df['CabinLetter'].cat.codes\n",
    "df['CabinLetter'] = df['CabinLetter'].replace(-1, np.NaN)\n",
    "print(df.CabinLetter.unique())\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression\n",
    "\n",
    "Logistic regression on every possible variable with plots for each of them.\n",
    "\n",
    "Plotting the logistic regression line has not been as easy as with the linear regression line.\n",
    "1. The original plot works, but I had to rewrite the formula in order for it to make sense. However, it has been designed to only give a straight line, so I will comment out its implementation.\n",
    "2. I implemented another plot with matplotlib (second plot). It doesn't quite work, and other people on the internet with the same problem have had the issue that their X were unordered. So I fixed that and ordered it, and now it works for all except 'Embarked'.\n",
    "3. Lastly, the third plot is an implementation using seaborn. It is the only implementation that does its own calculation of the results, while the other two rely on the results from the statsmodels.\n",
    "\n",
    "In summary, the second and third plot work. The first one works, but only for straight lines.\n",
    "\n",
    "Also, 'Embarked' is not visualized, since it is categorical with more than two values, so it doesn't work with these implementations. The logistic regression model still works though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665899\n",
      "         Iterations 4\n",
      "Modell für PassengerId:\n",
      "                 coef  std err      z  P>|z|\n",
      "intercept   -0.45540    0.138 -3.306  0.001\n",
      "PassengerId -0.00004    0.000 -0.149  0.881\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.608531\n",
      "         Iterations 5\n",
      "Modell für Pclass:\n",
      "              coef  std err      z  P>|z|\n",
      "intercept  1.4468    0.207  6.975    0.0\n",
      "Pclass    -0.8501    0.087 -9.755    0.0\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.672429\n",
      "         Iterations 4\n",
      "Modell für Age:\n",
      "              coef  std err      z  P>|z|\n",
      "intercept -0.0567    0.174 -0.327  0.744\n",
      "Age       -0.0110    0.005 -2.057  0.040\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.515041\n",
      "         Iterations 5\n",
      "Modell für Sex:\n",
      "              coef  std err       z  P>|z|\n",
      "intercept  1.0566    0.129   8.191    0.0\n",
      "Sex[male] -2.5137    0.167 -15.036    0.0\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.665272\n",
      "         Iterations 4\n",
      "Modell für SibSp:\n",
      "              coef  std err      z  P>|z|\n",
      "intercept -0.4382    0.076 -5.744  0.000\n",
      "SibSp     -0.0686    0.065 -1.050  0.294\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.662650\n",
      "         Iterations 4\n",
      "Modell für Parch:\n",
      "              coef  std err      z  P>|z|\n",
      "intercept -0.5531    0.077 -7.192  0.000\n",
      "Parch      0.2033    0.085  2.403  0.016\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.627143\n",
      "         Iterations 6\n",
      "Modell für Fare:\n",
      "              coef  std err      z  P>|z|\n",
      "intercept -0.9413    0.095 -9.895    0.0\n",
      "Fare       0.0152    0.002  6.809    0.0\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.636337\n",
      "         Iterations 4\n",
      "Modell für CabinLetter:\n",
      "                coef  std err      z  P>|z|\n",
      "intercept    0.6283    0.282  2.225  0.026\n",
      "CabinLetter  0.0272    0.101  0.269  0.788\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.650561\n",
      "         Iterations 4\n",
      "Modell für Embarked:\n",
      "                coef  std err      z  P>|z|\n",
      "intercept    0.2364    0.154  1.530  0.126\n",
      "Embarked[Q] -0.6853    0.280 -2.447  0.014\n",
      "Embarked[S] -0.9133    0.176 -5.203  0.000\n"
     ]
    }
   ],
   "source": [
    "def abline(ax, b, m, *args, **kwargs):\n",
    "    \"Add a line with slope m and intercept b to ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    #print(xlim)\n",
    "    ylim = [(np.e ** (m * xlim[0] + b)) / (1 + (np.e ** (m * xlim[0] + b))),\n",
    "            (np.e ** (m * xlim[1] + b)) / (1 + (np.e ** (m * xlim[1] + b)))]\n",
    "    #print(ylim)\n",
    "    #[m * xlim[0] + b, m * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args, **kwargs)\n",
    "    plt.show()\n",
    "\n",
    "for column in ['PassengerId', 'Pclass', 'Age', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter', 'Embarked']:\n",
    "    X = MS([column]).fit_transform(df)\n",
    "    model = sm.Logit(y,X,\n",
    "                   missing='drop')\n",
    "    results = model.fit()\n",
    "    print(\"Modell für {0}:\\n\".format(column), summarize(results))\n",
    "\n",
    "    if column == 'Embarked':\n",
    "        continue\n",
    "\n",
    "    # 1. first plot\n",
    "    #ax = df.plot.scatter(column, \"Survived\")\n",
    "    #abline(ax, results.params[0], results.params[1], 'r', linewidth=3)\n",
    "\n",
    "    # 2. second plot\n",
    "    \"\"\"sorted_X = X.sort_values(by=column)\n",
    "    plt.scatter(X[column], y)\n",
    "    plt.plot(sorted_X[column], results.predict(sorted_X), c=\"red\", linewidth=3)\n",
    "    plt.show()\n",
    "\n",
    "    # 3. third plot\n",
    "    sns.lmplot(x=column, y='Survived', data = df, logistic=True, ci=None)\n",
    "    plt.show()\"\"\"\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives very similar results to the simple linear regression. Like there, 'Pclass', 'Age', 'Sex', 'Parch', 'Fare' and 'Embarked' are statistically important.\n",
    "\n",
    "The next step is\n",
    "\n",
    "## Multiple logistic regression\n",
    "\n",
    "Like in multiple linear regression, on all possible columns at once. I also want to see whether the two implementations differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.423836\n",
      "         Iterations 7\n",
      "               coef  std err      z  P>|z|\n",
      "intercept    4.4131    1.228  3.593  0.000\n",
      "PassengerId  0.0016    0.001  1.991  0.046\n",
      "Pclass      -0.5621    0.491 -1.144  0.253\n",
      "Age         -0.0410    0.015 -2.775  0.006\n",
      "Sex[male]   -2.9607    0.503 -5.891  0.000\n",
      "SibSp        0.2647    0.360  0.736  0.462\n",
      "Parch       -0.3838    0.334 -1.148  0.251\n",
      "Fare         0.0010    0.003  0.330  0.742\n",
      "CabinLetter -0.0064    0.166 -0.038  0.969\n",
      "Embarked[Q] -1.8101    2.053 -0.882  0.378\n",
      "Embarked[S] -0.4125    0.472 -0.873  0.383\n",
      "               coef  std err      z  P>|z|\n",
      "intercept    4.4131    1.228  3.593  0.000\n",
      "PassengerId  0.0016    0.001  1.991  0.046\n",
      "Pclass      -0.5621    0.491 -1.144  0.253\n",
      "Age         -0.0410    0.015 -2.775  0.006\n",
      "Sex[male]   -2.9607    0.503 -5.891  0.000\n",
      "SibSp        0.2647    0.360  0.736  0.462\n",
      "Parch       -0.3838    0.334 -1.148  0.251\n",
      "Fare         0.0010    0.003  0.330  0.742\n",
      "CabinLetter -0.0064    0.166 -0.038  0.969\n",
      "Embarked[Q] -1.8101    2.053 -0.882  0.378\n",
      "Embarked[S] -0.4125    0.472 -0.873  0.383\n"
     ]
    }
   ],
   "source": [
    "X_2 = MS(['PassengerId', 'Pclass', 'Age', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter', 'Embarked']).fit_transform(df)\n",
    "model_2 = sm.Logit(y,X_2, missing = 'drop')\n",
    "results = model_2.fit()\n",
    "print(summarize(results))\n",
    "\n",
    "model_2_b = sm.GLM(y,X_2, missing = 'drop', family=sm.families.Binomial())\n",
    "results = model_2_b.fit()\n",
    "print(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation seems to be exactly the same. The [documentation](https://www.statsmodels.org/stable/generated/statsmodels.genmod.families.family.Binomial.html#statsmodels.genmod.families.family.Binomial) corroborates this, saying that logit is the default implementation of the Binomial model in the GLM models.\n",
    "\n",
    "I continue with sm.Logit for its shorter writing.\n",
    "\n",
    "Still significant are 'PassengerId' (that is expected to become less important as I delete variables from the model), 'Age' and 'Sex'. The last two are the same for both linear and logistic regression, but the linear regression also found 'Pclass' and 'SibSp' significant and not 'PassengerId'.\n",
    "\n",
    "Again, as the next step, I will drop the CabinLetter variable, since it has too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.442474\n",
      "         Iterations 6\n",
      "               coef  std err       z  P>|z|\n",
      "intercept    5.4851    0.657   8.350  0.000\n",
      "PassengerId  0.0004    0.000   0.937  0.349\n",
      "Pclass      -1.1971    0.165  -7.254  0.000\n",
      "Age         -0.0432    0.008  -5.257  0.000\n",
      "Sex[male]   -2.6574    0.223 -11.916  0.000\n",
      "SibSp       -0.3550    0.130  -2.735  0.006\n",
      "Parch       -0.0700    0.125  -0.562  0.574\n",
      "Fare         0.0015    0.003   0.578  0.563\n",
      "Embarked[Q] -0.8248    0.598  -1.379  0.168\n",
      "Embarked[S] -0.4067    0.271  -1.502  0.133\n"
     ]
    }
   ],
   "source": [
    "X_2 = MS(['PassengerId', 'Pclass', 'Age', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']).fit_transform(df)\n",
    "model_2 = sm.Logit(y,X_2, missing = 'drop')\n",
    "results = model_2.fit()\n",
    "print(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking much better. PassengerId is not important anymore, as we would expect. Pclass and SibSp have become significant. Now, all the same variables to the linear model are significant again.\n",
    "\n",
    "I have done this quite late in the linear regression, but I will now replace all the missing values in 'Age' with their mean (a step that should normally happen at the start while cleaning the columns and handling missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df['Age'])\n",
    "df['AgeNoMissing'] = df['Age'].fillna(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll do backwards selection on the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.443796\n",
      "         Iterations 6\n",
      "                coef  std err       z  P>|z|\n",
      "intercept     5.1920    0.478  10.854  0.000\n",
      "Pclass       -1.1724    0.120  -9.792  0.000\n",
      "AgeNoMissing -0.0398    0.008  -5.131  0.000\n",
      "Sex[male]    -2.7398    0.194 -14.112  0.000\n",
      "SibSp        -0.3578    0.104  -3.439  0.001\n"
     ]
    }
   ],
   "source": [
    "X_2 = MS(['Pclass', 'AgeNoMissing', 'Sex', 'SibSp']).fit_transform(df)\n",
    "model_2 = sm.Logit(y,X_2, missing = 'drop')\n",
    "results = model_2.fit()\n",
    "print(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant are:\n",
    "\n",
    "- Pclass\n",
    "- Age\n",
    "- Sex\n",
    "- SibSp\n",
    "\n",
    "The same as with multiple linear regression.\n",
    "\n",
    "## Multiple Logistic Regression with interaction\n",
    "\n",
    "I will directly include all possible interaction terms (except for PassengerId since it's obviously irrelevant and Embarked, since they are categorical with more than two categories) and then do backwards selection. Of the non-interaction terms, I'll only include the four significant ones in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.408473\n",
      "         Iterations 8\n",
      "                       coef  std err      z  P>|z|\n",
      "intercept            4.1440    1.601  2.589  0.010\n",
      "Pclass              -1.1619    0.521 -2.229  0.026\n",
      "AgeNoMissing         0.0251    0.041  0.605  0.545\n",
      "Sex[male]           -3.7514    1.502 -2.497  0.013\n",
      "SibSp                1.9061    1.085  1.756  0.079\n",
      "Pclass:AgeNoMissing -0.0140    0.014 -1.007  0.314\n",
      "Pclass:SexNr         0.8609    0.445  1.934  0.053\n",
      "Pclass:SibSp        -0.8109    0.309 -2.627  0.009\n",
      "Pclass:Parch         0.1700    0.121  1.408  0.159\n",
      "Pclass:Fare          0.0075    0.007  1.144  0.252\n",
      "AgeNoMissing:SexNr  -0.0444    0.021 -2.070  0.038\n",
      "AgeNoMissing:SibSp   0.0031    0.013  0.234  0.815\n",
      "AgeNoMissing:Parch  -0.0252    0.010 -2.400  0.016\n",
      "AgeNoMissing:Fare    0.0003    0.000  1.064  0.287\n",
      "SexNr:SibSp         -0.1358    0.311 -0.437  0.662\n",
      "SexNr:Parch          0.9007    0.303  2.971  0.003\n",
      "SexNr:Fare          -0.0122    0.011 -1.155  0.248\n",
      "SibSp:Parch         -0.0088    0.184 -0.048  0.962\n",
      "SibSp:Fare          -0.0078    0.005 -1.586  0.113\n",
      "Parch:Fare          -0.0034    0.004 -0.955  0.340\n"
     ]
    }
   ],
   "source": [
    "X_3 = MS(['Pclass', 'AgeNoMissing', 'Sex', 'SibSp']).fit_transform(df)\n",
    "\n",
    "items = ['Pclass', 'AgeNoMissing', 'SexNr', 'SibSp', 'Parch', 'Fare']\n",
    "for first in range(len(items)):\n",
    "    for second in range(len(items)):\n",
    "        if first >= second: continue\n",
    "        title = items[first] + ':' + items[second]\n",
    "        X_3[title] = df[items[first]] * df[items[second]]\n",
    "model_3 = sm.Logit(y,X_3)\n",
    "results = model_3.fit()\n",
    "print(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, dropping all non-significant predictors one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.412587\n",
      "         Iterations 8\n",
      "                      coef  std err      z  P>|z|\n",
      "intercept           5.6628    0.931  6.085  0.000\n",
      "Pclass             -1.7635    0.330 -5.351  0.000\n",
      "Sex[male]          -5.0395    1.069 -4.713  0.000\n",
      "SibSp               2.1085    0.690  3.056  0.002\n",
      "Pclass:SexNr        1.1316    0.348  3.250  0.001\n",
      "Pclass:SibSp       -0.8464    0.230 -3.685  0.000\n",
      "Pclass:Parch        0.1982    0.097  2.034  0.042\n",
      "Pclass:Fare         0.0043    0.002  1.753  0.080\n",
      "AgeNoMissing:SexNr -0.0302    0.011 -2.732  0.006\n",
      "AgeNoMissing:Parch -0.0271    0.008 -3.489  0.000\n",
      "SexNr:Parch         0.6612    0.244  2.707  0.007\n",
      "SibSp:Fare         -0.0085    0.003 -2.997  0.003\n"
     ]
    }
   ],
   "source": [
    "dropping = ['SibSp:Parch', 'AgeNoMissing:SibSp', 'AgeNoMissing:Fare', 'SexNr:Fare',\n",
    "            'SexNr:SibSp', 'Parch:Fare', 'AgeNoMissing', 'Pclass:AgeNoMissing']\n",
    "for item in dropping:\n",
    "    try:\n",
    "        X_3 = X_3.drop(item, axis=1)\n",
    "    except:\n",
    "        continue\n",
    "model_3 = sm.Logit(y,X_3)\n",
    "results = model_3.fit()\n",
    "print(summarize(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Multiple Logistic Regression without interaction terms:\n",
    "\n",
    "- Significant were Pclass, Sex, Age, SibSp\n",
    "\n",
    "Results of Multiple Logistic Regression without itneraction terms:\n",
    "\n",
    "- Significant are Pclass, Sex, SibSp, Pclass:SexNr, Pclass:SibSp, Pclass:Parch, Pclass:Fare, AgeNoMissing:SexNr, AgeNoMissing:Parch, SexNr:Parch, SibSp:Fare\n",
    "- AgeNoMissing has become insignificant\n",
    "\n",
    "## Multiple Logistic Regression with Polynomial Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
